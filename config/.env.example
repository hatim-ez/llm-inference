# Model Configuration
MODEL_NAME=Llama-3.2-11B-Vision-Instruct
MODEL_PATH=/home/ubuntu/models/llama-3.2-11b-vision

# vLLM Configuration
TENSOR_PARALLEL_SIZE=1
GPU_MEMORY_UTILIZATION=0.90
MAX_MODEL_LENGTH=4096
MAX_NUM_SEQS=128

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
RATE_LIMIT=100
REQUEST_TIMEOUT=300
REQUIRE_API_KEY=false
API_KEY_SECRET=your-secret-key-here

# Monitoring
ENABLE_METRICS=true
LOG_LEVEL=INFO
LOG_FORMAT=json

# AWS Configuration (optional)
AWS_REGION=us-east-1
S3_BUCKET=llm-inference-logs
