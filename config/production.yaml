# Production Configuration for LLM Inference System

model:
  name: "Llama-3.2-11B-Vision-Instruct"
  path: "/home/ubuntu/models/llama-3.2-11b-vision"
  max_model_length: 4096

vllm:
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.90
  max_num_seqs: 128
  enforce_eager: false
  swap_space: 4  # GB

api:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  timeout: 300
  max_batch_size: 32

security:
  require_api_key: false
  rate_limit:
    requests_per_minute: 100
    burst_size: 20

logging:
  level: "INFO"
  format: "json"
  file: "/home/ubuntu/logs/llm-inference.log"
  max_size_mb: 100
  backup_count: 5

monitoring:
  enabled: true
  prometheus_port: 9090
  health_check_interval: 30

performance:
  batch_timeout_ms: 50
  max_concurrent_requests: 64
  request_queue_size: 256
