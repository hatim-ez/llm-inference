# Prometheus Configuration for LLM Inference Monitoring

global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    monitor: 'llm-inference'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets: []
          # - alertmanager:9093

# Rule files
rule_files:
  - "alerts/*.yml"

# Scrape configurations
scrape_configs:
  # Prometheus self-monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
    metrics_path: /metrics
    scheme: http

  # LLM Inference API metrics
  - job_name: 'llm-api'
    static_configs:
      - targets: ['host.docker.internal:8000']
    metrics_path: /metrics
    scheme: http
    scrape_interval: 10s
    scrape_timeout: 5s

  # Node Exporter - System metrics
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
    scrape_interval: 15s

  # DCGM Exporter - GPU metrics
  - job_name: 'dcgm-exporter'
    static_configs:
      - targets: ['dcgm-exporter:9400']
    scrape_interval: 5s
    metrics_path: /metrics

  # Nginx (if enabled)
  - job_name: 'nginx'
    static_configs:
      - targets: ['host.docker.internal:9113']
    scrape_interval: 30s
